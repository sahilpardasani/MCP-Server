# MCP-Server

 This project sets up a dual-mode Model Context Protocol (MCP) server that supports both:

ğŸ§© Tool-based responses (e.g., current time, jokes) via FastMCP
ğŸ’¬ Prompt-response LLM completions via FastAPI


## ğŸ“ Project Structure

| File        | Purpose                                      |
|-------------|----------------------------------------------|
| `server.py` | Handles LLM loading, generation, config      |


ğŸš€ Features

âš™ï¸ Compatible with Claude Desktop, MCP Inspector, LangGraph, etc.
ğŸ”Œ FastMCP standard for tool registration and stdin communication
ğŸ§  Run local LLM completions from app.py using /generate API
ğŸŒ Optional HTTP server mode for broader integrations
ğŸ§ª MCP tools return structured JSON responses

ğŸ›  Installation
pip install fastapi uvicorn transformers torch
pip install requests
pip install bitsandbytes accelerate  # Only needed if using quantized models

ğŸ›  Registered MCP Tools
Tool Name	  What It Does
time_now	  Returns current UTC timestamp
dad_joke	  Returns a random dad joke from API

ğŸ’¡ What is MCP?
Model Context Protocol (MCP) allows custom tools and local models to be integrated into AI assistants like Claude Desktop or LangGraph. You can register Python functions as tools, and they become callable by the assistant when relevant.

<img width="1787" alt="Screenshot 2025-05-27 at 1 21 35â€¯PM" src="https://github.com/user-attachments/assets/4001416f-9a59-4507-91c1-83798c9d17c8" />
<img width="764" alt="Screenshot 2025-06-06 at 12 36 34â€¯PM" src="https://github.com/user-attachments/assets/d1da2cbd-731e-468d-9e82-729967801d07" />

